+++ 
draft = false
date = 2025-02-07T21:53:44-06:00
title = "El Dilema de la Habitación China: Los LLM's actuales sin Conciencia ni Comprensión"
description = "Análisis de los límites de la Inteligencia Artificial derivado del experimento de la Habitación China"
slug = ""
authors = []
tags = []
categories = ["Artificial Intelligence"]
externalLink = ""
series = []
+++

Desde la década de los 60's, ante el nacimiento del concepto de Inteligencia Artificial como algo más allá de la utopía fantástica, se ha debatido sobre la proyección y concepción de una verdadera Inteligencia aterrizada en lo Artificial. Han nacido paradojas, teorías, argumentos y cadenas de razonamientos que en conjunto intentan definir el futuro de este campo. 

Como mencioné en el post sobre [La carencia de Pensamiento en la IA actual](../../posts/thinking-simulation), es difícil hablar del futuro de esta tecnología ante una verdadera Inteligencia, pues el mismo concepto y concepción de Inteligencia presenta desafíos. Sin embargo, quiero presentar el desafío implícito detrás del **Dilema de la Habitación China** ante las capacidades cognitivas de los Modelos Grandes de Lenguaje (LLM's por sus siglas al inglés).

# Modelos Grandes de Lenguaje (LLM's)
Antes de introducir a La Prueba de la Habitación China a los Modelos Grandes de Lenguaje, daré una explicación de lo que son, en esencia, los Modelos Grandes de Lenguaje.
Los LLM's son modelos de "Inteligencia Artificial" entrenados para **procesar y generar texto de manera coherente**, basandose en redes neuronales profundas, especialmente en la [Arquitectura Transformer](../../posts/transformer-architecture). Los LLM's son entrenados con enormes volúmenes de valores para su funcionamiento. Por considerar algunos (en valores aproximados):
- **GPT-4**: 1-2 Trillones de parámetros (2-3.6 TeraBytes)
- **Gemini 1 Ultra**: 1 Trillón de parámetros (1-2 TeraBytes)
- **GPT-3**: 175 Billones de parámetros (350 GigaBytes)
- **LLaMA**: 65 Billones de parámetros (130 GigaBytes)

# La Prueba de la Habitación China
En 1980, John Searle, Filósofo y profesional de la Mente y del Lenguaje, propuso la llamada _Prueba de la Habitación China_: Consideremos a un programa computado capaz de dialogar con una persona nativa del lenguaje Chino. La máquina es capaz de sostener una conversación escrita con dicha persona mediante un organigrama, manipulando los caracteres del mandarín. La máquina se encuentra encerrada en una habitación, esto con la intención de que la persona no pueda verlo. La capacidad de la máquina ante el diálogo en mandarín con la persona es suficiente para hacer que dicha persona no logre percibir que está dialogando con una máquina, y no con una persona. Se dice que el programa pasó la [La Prueba de Turing](../../posts/turing-test).

Ahora, consideremos a una _X_ persona, digámosle Juan. Juan, sin saber absolutamente nada de mandarín, entra a la habitación para sustituir a la máquina, y hace uso del organigrama que la máquina utilizó para continuar la conversación con el nativo Chino. En principio, Juan debería ser capaz de sostener satisfactoriamente la conversación con el nativo Chino, haciendo uso de únicamente el organigrama y las reglas del mandarín. A pesar de esto, Juan es totamente **consciente** de su ignorancia ante el mandarín, aunque la conversación haya sido coherente, Juan no ha entendido absolutamente nada de lo que ha emitido desde el organigrama. 

La cuestión es, ¿la máquina entiende la conversación que tuvo con la mujer?, y si no la entiende, ¿la máquina es consciente de que no ha entendido?...

# De la Conciencia
La conciencia implica tener una **experiencia subjetiva** y la capacidad de **reflexionar** sobre el propio estado mental. Para saber si un LLM tiene conciencia, debemos preguntarnos: ¿Es consciente de sus acciones o respuestas?

Los LLMs como GPT-4 no tienen conciencia de sí mismos ni de lo que están haciendo. Generan respuestas basadas en patrones estadísticos sin tener una experiencia subjetiva; _no hay self_.

La máquina dentro de la habitación puede sostener una conversación en chino, pero no sabe lo que está diciendo. No tiene una "experiencia" de lo que está generando, simplemente está siguiendo reglas predefinidas. De la misma manera, un LLM no tiene conciencia de sus respuestas. Aunque parece interactuar de manera coherente, no tiene un sentido interno de lo que está diciendo.

# De la Comprensión
La comprensión implica no solo procesar información, sino también **asignar significado** a esa información de manera profunda.
Los LLMs no comprenden el texto que generan, al igual que Juan no entiende chino (ya sé, el mandarín). Los modelos manipulan símbolos y generan texto basado en patrones aprendidos, sin tener un entendimiento real. Hace falta **intencionalidad** y **significado consciente**. 

-------

Mientras un modelo de LLM sea incapaz de tener un estado mental ante un contexto, y relacionar su experiencia para poder reflexionar ante dicho contexto, mantendré la postura de que cualquier LLM, por colosal, capaz y fascinante que sea en su arquitectura, será incapaz de nivelarse con la esencia del Lenguaje Humano. 

A pesar de esto, encuentro importante una pregunta subjetiva que puede romper todo mi análisis: **Juan necesitó un poco de su Inteligencia para saber interpretar y usar el organigrama y las reglas del mandarín, ¿cierto?**...
