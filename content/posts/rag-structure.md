+++ 
draft = false
date = 2025-04-17T22:11:49-06:00
title = "dRAGan: Proyecto de IA por Generaci√≥n Aumentada por Recuperaci√≥n (RAG)"
description = ""
slug = "1"
authors = []
tags = []
categories = ["LLM's", "RAG"]
externalLink = ""
series = []
+++
Recientemente, a trav√©s de una asignatura en la Facultad, tuve la oportunidad de profundizar en el estudio te√≥rico-pr√°ctico de Redes Neuronales, con especial enfoque en los **Modelos de Lenguaje a Gran Escala (LLMs)** (como lo es GPT, Gemini, LlaMa, Claude, etc). Entre los temas abordados, destaco el an√°lisis de t√©cnicas de Inteligencia Artificial Generativa aplicadas al **Procesamiento del Lenguaje Natural (PLN)**, en particular la Generaci√≥n Aumentada por Recuperaci√≥n (RAG), un paradigma que combina la capacidad generativa de los LLMs con la recuperaci√≥n contextual de informaci√≥n externa. 

Para el final del curso, se nos pidi√≥ hacer un proyecto relacionado a los temas vistos en clase, a lo cual me remit√≠ a la arquitectura RAG para dise√±ar un agente capaz de, a partir de archivos de tipo PDF o de texto (.txt), generar respuestas concisas y precisas a la pregunta del usuario bas√°ndose √∫nicamente en la informaci√≥n que contienen los documentos. Si quieres consultar m√°s detalladamente el proyecto, lo encuentras aqu√≠: [dRAGan: Agente de IA por t√©cnica RAG](../../projects/dragan) (aunque vale la pena decir que con este proyecto tuve m√°s la intenci√≥n de aprender, que de generar algo √∫til m√°s all√° de otras tecnolog√≠as).

As√≠ pues, ser√≠a de mi gusto explicar el funcionamiento del proyecto, recordando que esta secci√≥n de publicaciones tambi√©n la uso para mi estudio propio.

# üíª Funcionamiento final del Agente

#### Entrada de Datos
- Recibe documentos en formatos PDF (.pdf) y/o texto plano (.txt)  
**Caso de uso**: carpeta con art√≠culos m√©dicos sobre Tuberculosis.

#### Consulta del Usuario
- Procesa preguntas en lenguaje natural.  
**Caso de uso**: ¬øQu√© es la Tuberculosis?

#### Procesamiento de la Respuesta
- **B√∫squeda contextual**: Extrae informaci√≥n relevante SOLO de los documentos.
- **Generaci√≥n**: Crea respuestas concisas basadas en el contexto recuperado.
- **Manejo de errores**: Si no se encontr√≥ informaci√≥n suficiente (el agente necesita al menos 30% de contexto √∫til para generar la respuesta), invita al usuario a a√±adir m√°s informaci√≥n.

Este funcionamiento garantiza que la respuesta sea generada bas√°ndose √∫nicamente en los documentos proporcionados, evitando ambig√ºedades y asegurando veracidad en la respuesta (la veracidad depende por supuesto de la veracidad de los documentos).


# üß† Funcionamiento interno/matem√°tico del Agente

#### Tokenizaci√≥n
El Agente utiliza un **tokenizador especializado** (como WordPiece o BPE) para dividir el texto en unidades significativas (como separar una receta en ingredientes individuales), y almacenar los tokens en un vector (imagina a un vector como una colecci√≥n de elementos), por ejemplo:  

- Informaci√≥n del documento: `"Tuberculosis enfermedad infecciosa" ‚Üí ["Tuberculosis", "enfemedad", "infecciosa"]` 
- Pregunta: `"¬øQu√© es la Tuberculosis?" ‚Üí ["¬ø","Qu√© es","la","Tuberculosis","?"]`   
Este ejemplo es demasiado pobre, pero es importante reconocer que al _tokenizarse_ cada archivo, se generan vectores con demasiados elementos.

#### Transformaci√≥n a Embeddings
Un _Embedding_ es una representaci√≥n num√©rica de una palabra o frase. Mediante una estructura de datos llamada **diccionario**, cada token del vector se "mapea" a un n√∫mero. por ejemplo:  

- Informaci√≥n del documento:   `["Tuberculosis", "enfermedad", "infecciosa"] ‚Üí [5.3, 0.24, -3.12]`
- Pregunta: `["¬ø","Qu√© es","la","Tuberculosis","?"] ‚Üí [-6.45, 21.3, 3.2, 5.3, -6.23]`  

La estructura de datos Diccionario garantiza que el mapeo num√©rico de un elemento siempre sea el mismo (suponiendo el diccionario que yo us√©, el token "Tuberculosis" siempre se mapear√° al n√∫mero 5.3).

#### B√∫squeda Sem√°ntica (√Ålgebra Lineal en B√∫squeda Vectorial)
Calcula la **similitud coseno** entre los vectores de la pregunta y los fragmentos del documento. Esta medida indica qu√© tan cercanos (en significado) son dos vectores, siendo 1.0 (100%) la m√°xima similitud posible
```bash {class="my-class" id="my-codeblock" lineNos=inline tabWidth=2}
similitud = (vector_pregunta ‚Ä¢ vector_documento) / (||vector_pregunta|| * ||vector_documento||)

```
_dRAGan_ selecciona los 3 fragmentos con mayor similitud y se almacenan como contexto relevante.

#### Recuperaci√≥n del contexto
El agente concatena los fragmentos m√°s relevantes en un s√≥lo bloque de texto, que ser√° utilizado como _fuente de conocimiento_ para responder. Este paso se conoce como **context retrieval** y es clave en el enfoque RAG: el modelo no responde desde su entrenamiento general, sino desde lo que recupere.

#### Generaci√≥n de la Respuesta 
_dRAGan_ le hace una petici√≥n a un modelo de lenguaje (LLM), en este caso Phi3 Mini, para que construya la respuesta a la pregunta del usuario bas√°ndose en el contexto recuperado. La generaci√≥n de la respuesta es condicionada, es decir, est√° influenciada directamente por la informaci√≥n proporcionada. Antes de hacer la petici√≥n al LLM, el agente verifica que se haya encontrado al menos 30% de similitud promedio entre los fragmentos y la pregunta, si no se da el caso, el agente directamente devuelve la queja de que no se encontr√≥ informaci√≥n suficiente, invitiando al usuario a a√±adir m√°s documentos.


_____
Este proyecto me permiti√≥ entender en la pr√°ctica c√≥mo un LLM puede ser m√°s controlable y preciso si lo conectamos con un mecanismo de recuperaci√≥n (RAG), evitando respuestas inventadas (alucinaciones) t√≠picas de los modelos generativos puros.

Adem√°s, reafirm√© la importancia del preprocesamiento, el manejo de vectores, y la integraci√≥n efectiva entre IA generativa y t√©cnicas cl√°sicas como la b√∫squeda vectorial.
